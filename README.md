# ğŸ™ï¸ LectureMate

LectureMate is a popup window for an intelligent voice-based lecture evaluator that enables real-time lecture analysis, question generation, and performance feedback using AI and speech recognition.

![LectureMate Logo](static/images/logo.png)

---

## ğŸ“Œ Features

* ğŸ—£ï¸ **Voice Recording**: Capture lecture audio directly from your browser via a popup interface.
* ğŸ“œ **Transcription**: Convert spoken words into text using Google's Speech Recognition API.
* ğŸ§  **AI Question Generator**: Auto-generate concise, relevant questions based on lecture content using OpenAI's GPT 4.1.
* âœ… **Answer Evaluation**: Submit answers and receive instant feedback and a grade.
* ğŸ’¾ **Session Storage**: Save transcripts, questions, and feedback to MongoDB for review and analysis.

---

## ğŸš€ Getting Started

### Prerequisites

* Python 3.8+
* Node.js & npm (for frontend assets, optional)
* MongoDB Atlas or local MongoDB instance
* OpenAI API token

### Installation

1. **Clone the repository**:

   ```bash
   git clone https://github.com/yourusername/lecturemate.git
   cd lecturemate
   ```

2. **Create and activate a virtual environment**:

   ```bash
   python3 -m venv venv
   source venv/bin/activate      # on Linux/Mac
   venv\\Scripts\\activate     # on Windows
   ```

3. **Install Python dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

4. **Set environment variables** in a `.env` file:

   ```dotenv
   MONGODB_URI=<your-mongodb-uri>
   HUGGINGFACE_TOKEN=<your-hf-token>
   USER_ID=your_user_id
   SESSION_ID=unique_session_id
   MODEL_NAME=valhalla/t5-small-qg-hl
   ```

5. **Run the Flask app**:

   ```bash
   python run.py
   ```

6. **Open in browser**:

   Navigate to `http://127.0.0.1:5000/home` and click **Start LectureMate** to start recording.

   **User Data**
   * You can also add ?username=<your-username> to save it for a specific user
   * While on a specific user you can see your sessions feedbacks and grades.

---

## ğŸ—‚ï¸ Project Structure

```
LectureMate/
â”œâ”€â”€ app/                 # Core application package
â”‚   â”œâ”€â”€ __init__.py      # App factory
â”‚   â”œâ”€â”€ routes.py        # Flask route definitions
â”‚   â”œâ”€â”€ voice_analyzer.py# VoiceRecorder class and logic
â”‚   â””â”€â”€ user_data.py         # User data
â”‚
â”œâ”€â”€ static/              # Static assets
â”‚   â”œâ”€â”€ images/          # Logo and promo images
â”‚   â”‚   â”œâ”€â”€ logo.png
|   â”‚   â”œâ”€â”€ example_home_page.png
â”‚   â”‚   â””â”€â”€ favicon.PNG
â”‚   â”œâ”€â”€ popup.js         # Frontend popup logic
|   â”œâ”€â”€ user-grades.css # Styles for feedbacks page
â”‚   â””â”€â”€ styles.css       # Styles for main page
â”‚
â”œâ”€â”€ templates/           # HTML templates
â”‚   â”œâ”€â”€ layout.html      # Base template
â”‚   â”œâ”€â”€ user-grades.html # Feedbacks page
â”‚   â””â”€â”€ home.html       # Popup launcher page
â”‚
â”œâ”€â”€ tests/               # Unit tests
â”‚   â”œâ”€â”€ test_import.py # Test core logic
â”‚   â”œâ”€â”€ test_model.py # Test prompt using Hugging-Face models for questions and feedbacks (Prompt Engineering)
â”‚   â”œâ”€â”€ test_quete_gen.py # Test prompt using OpenAI API for questions and feedbacks (Prompt Engineering)
â”‚   â””â”€â”€ test_whisper.py # Test a second option for analyzing audio - using whisper API (require GPU and model storage)
â”‚
â”œâ”€â”€ .env                 # Environment variables
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ run.py               # App entry point
â””â”€â”€ README.md            # Project documentation
```

---

## âš™ï¸ Usage

1. Click **Start Session** to begin capturing audio.
2. Use **Pause/Resume** to manage recording segments.
3. **Answer Question** processes and submits your answer for AI feedback.
4. **Stop Recording** ends the session and saves data to MongoDB.
5. **User Grades** opens the user's sessions feedbacks.

---

## ğŸ“ Contributing

Contributions are welcome! Please open an issue or submit a pull request.

---

## ğŸ“œ License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.
